{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries and set paths","metadata":{"_uuid":"243b4407-21b4-4cd5-93ff-86d56a29accb","_cell_guid":"20b17800-ad2b-4d3f-a03c-09b5dc0e2868","trusted":true}},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport numpy as np\nimport cv2\nimport csv\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch import nn\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom tqdm import tqdm\nimport pandas as pd\n\n# Set the path for the data\nDATA_PATH = '/kaggle/input/vesuvius-challenge-ink-detection'\nTRAIN_PATH = os.path.join(DATA_PATH, 'train')\nTEST_PATH = os.path.join(DATA_PATH, 'test')","metadata":{"_uuid":"e9f121a7-5ce9-42c1-9525-cf43b7e8667b","_cell_guid":"067171e0-4599-4144-92ef-89122c8e85b6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:02.838124Z","iopub.execute_input":"2023-05-12T08:20:02.838564Z","iopub.status.idle":"2023-05-12T08:20:02.846898Z","shell.execute_reply.started":"2023-05-12T08:20:02.838529Z","shell.execute_reply":"2023-05-12T08:20:02.845679Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Constants","metadata":{"_uuid":"c9d40090-a05d-4e10-90f2-b5047d80a20a","_cell_guid":"7fa3c8eb-eea0-4aba-954c-ea5fb6f4ec79","trusted":true}},{"cell_type":"code","source":"PREFIX = '/kaggle/input/vesuvius-challenge-ink-detection/train/1/'\nBUFFER = 30\nZ_START = 27\nZ_DIM = 10\nTRAINING_STEPS = 30000\nNUM_INK_PIXELS = 100\nLEARNING_RATE = 0.001\nBATCH_SIZE = 16\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8ac6a5d3-483b-42c3-911e-81bff0056139","_cell_guid":"8e26ef8e-8fbf-4f9e-94f2-bccc9038c35f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:02.849225Z","iopub.execute_input":"2023-05-12T08:20:02.849619Z","iopub.status.idle":"2023-05-12T08:20:02.863493Z","shell.execute_reply.started":"2023-05-12T08:20:02.849587Z","shell.execute_reply":"2023-05-12T08:20:02.862333Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Define SubvolumeDataset class","metadata":{"_uuid":"d5b76e83-4252-49d3-b430-1147f6cd5e58","_cell_guid":"d1dc5ffd-bfa6-4990-859a-5330824a7b17","trusted":true}},{"cell_type":"code","source":"class SubvolumeDataset(Dataset):\n    def __init__(self, data_paths, mask_paths, label_paths=None, device='cpu'):\n        self.data_paths = data_paths\n        self.mask_paths = mask_paths\n        self.label_paths = label_paths\n        self.device = device\n\n    def __len__(self):\n        return len(self.data_paths)\n\n    def __getitem__(self, idx):\n        data_path = self.data_paths[idx]\n        mask_path = self.get_corresponding_mask_path(data_path)\n        print(\"Data path:\", data_path)\n        print(\"Mask path:\", mask_path)\n\n        data = cv2.imread(data_path, cv2.IMREAD_GRAYSCALE)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) if mask_path is not None else None\n        if mask is None:\n            mask = np.zeros_like(data)\n        print(\"Mask shape:\", mask.shape)\n\n        # Resize mask to a fixed size\n        desired_size = (data.shape[1], data.shape[0])\n        mask = cv2.resize(mask, desired_size)\n\n        if self.label_paths:\n            label_path = self.get_corresponding_label_path(data_path)\n            label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n        else:\n            label = None\n\n        data_tensor = torch.from_numpy(data).unsqueeze(0).to(self.device)\n        mask_tensor = torch.from_numpy(mask).unsqueeze(0).to(self.device)\n        label_tensor = torch.from_numpy(label).unsqueeze(0).to(self.device) if label is not None else None\n\n        return data_tensor, mask_tensor, label_tensor\n\n    def get_corresponding_mask_path(self, data_path):\n        # Generate the corresponding mask path\n        mask_path = data_path.replace('surface_volume', 'mask')\n        return mask_path\n\n    def get_corresponding_label_path(self, data_path):\n        # Generate the corresponding label path\n        label_path = data_path.replace('surface_volume', 'inklabels')\n        return label_path","metadata":{"_uuid":"7bd608c1-ae3d-40ec-9299-0b26f668847f","_cell_guid":"74318335-9da0-45bc-b50a-6e8b5629f7fb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:21:31.099481Z","iopub.execute_input":"2023-05-12T08:21:31.100664Z","iopub.status.idle":"2023-05-12T08:21:31.114513Z","shell.execute_reply.started":"2023-05-12T08:21:31.100621Z","shell.execute_reply":"2023-05-12T08:21:31.112826Z"},"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[20], line 12\u001b[0;36m\u001b[0m\n\u001b[0;31m    data_path = self.data_paths[idx]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 11\n"],"ename":"IndentationError","evalue":"expected an indented block after function definition on line 11 (1714059952.py, line 12)","output_type":"error"}]},{"cell_type":"markdown","source":"## Initialize Parameters","metadata":{"_uuid":"5e288725-7b60-40f0-8ee5-9b51fd301fca","_cell_guid":"9c3a12cc-3361-4b56-b93a-4bbcddf3fe80","trusted":true}},{"cell_type":"code","source":"# Number of input channels for the model\nin_channels = 1  # Assuming grayscale images, adjust as per your data\n\n# Number of training epochs\nEPOCHS = 10  # Adjust as per your requirement\n\n# Threshold for the RLE encoding\nTHRESHOLD = 0.5  # Adjust based on your model's output","metadata":{"_uuid":"52fc66e6-e153-45c2-9525-b2ec917437b5","_cell_guid":"6b53ed87-8724-464a-ba23-32734cf45870","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:02.879758Z","iopub.execute_input":"2023-05-12T08:20:02.880346Z","iopub.status.idle":"2023-05-12T08:20:02.892992Z","shell.execute_reply.started":"2023-05-12T08:20:02.880316Z","shell.execute_reply":"2023-05-12T08:20:02.891913Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Define the paths to the training and test data","metadata":{"_uuid":"4b487716-7e80-4d56-952b-25cb71afd439","_cell_guid":"07d556c9-14db-4066-94bf-5c4d0deffef5","trusted":true}},{"cell_type":"code","source":"# For the training data, we have both input (surface volume) and labels (inklabels)\ntrain_data_paths = sorted(glob.glob(os.path.join(TRAIN_PATH, '*/surface_volume/*.tif')))\ntrain_label_paths = sorted(glob.glob(os.path.join(TRAIN_PATH, '*/inklabels.png')))\n\n# For the test data, we only have the input (surface volume)\ntest_data_paths = sorted(glob.glob(os.path.join(TEST_PATH, '*/surface_volume/*.tif')))\n\n# Now, we create the datasets:\ntrain_dataset = SubvolumeDataset(train_data_paths, train_label_paths, device=DEVICE)\ntest_dataset = SubvolumeDataset(test_data_paths, [], device=DEVICE)","metadata":{"_uuid":"c1f97d72-644a-40f1-be08-29c4b26e2a13","_cell_guid":"15db4a65-e51c-4d8b-b126-23993dfe937f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:02.896064Z","iopub.execute_input":"2023-05-12T08:20:02.896537Z","iopub.status.idle":"2023-05-12T08:20:02.921915Z","shell.execute_reply.started":"2023-05-12T08:20:02.896493Z","shell.execute_reply":"2023-05-12T08:20:02.920980Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Preparation","metadata":{"_uuid":"98a72a2f-dfd0-4536-803b-36cf693667de","_cell_guid":"1c546a5d-cfa8-414d-9354-03bbd8fc4cda","trusted":true}},{"cell_type":"code","source":"vol_dims = (512, 512, 512)  # Replace with the actual dimensions\ntrain_dataset = SubvolumeDataset(train_data_paths, train_label_paths, device=DEVICE)\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\ntest_dataset = SubvolumeDataset(test_data_paths, [], device=DEVICE)","metadata":{"_uuid":"ee42e42a-a5ec-465d-a365-cbc33d730477","_cell_guid":"7e2c5b79-1d3c-4c94-8a3c-d41ac1163d86","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:02.923461Z","iopub.execute_input":"2023-05-12T08:20:02.924065Z","iopub.status.idle":"2023-05-12T08:20:02.929352Z","shell.execute_reply.started":"2023-05-12T08:20:02.924034Z","shell.execute_reply":"2023-05-12T08:20:02.928582Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Model Def","metadata":{"_uuid":"c66fd147-397c-4f61-b6ea-3f8c027f2457","_cell_guid":"54407f9f-5699-4614-8d2b-b0027b0c21ad","trusted":true}},{"cell_type":"code","source":"class SegmentationModel(nn.Module):\n    def __init__(self, in_channels=1):\n        super(SegmentationModel, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n\n    def forward(self, x):\n        # Assuming x is of shape [batch_size, channels, height, width]\n        x1 = F.relu(self.conv1(x))\n        x2 = F.relu(self.conv2(x1))\n        x3 = F.relu(self.conv3(x2))\n        x4 = F.relu(self.conv4(x3))\n        x5 = F.relu(self.conv5(x4))\n        \n        x6 = F.relu(self.upconv1(x5))\n        x7 = F.relu(self.upconv2(x6))\n        x8 = F.relu(self.upconv3(x7))\n        x9 = F.relu(self.upconv4(x8))\n        \n        output = self.final_conv(x9)\n        \n        return output","metadata":{"_uuid":"7890c1b7-77d1-4e35-9f2a-b52208563486","_cell_guid":"0ca48119-3d29-4154-a214-f119158931c1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:02.930908Z","iopub.execute_input":"2023-05-12T08:20:02.931317Z","iopub.status.idle":"2023-05-12T08:20:02.946692Z","shell.execute_reply.started":"2023-05-12T08:20:02.931287Z","shell.execute_reply":"2023-05-12T08:20:02.945283Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation Metrics","metadata":{"_uuid":"0ffa259c-6dee-4f47-a85f-0b59750e3e7a","_cell_guid":"04fc24ec-6298-48fe-b6af-b1ca84d9b818","trusted":true}},{"cell_type":"code","source":"def dice_coefficient(y_true, y_pred):\n    smooth = 1e-5\n    y_true_f = y_true.view(-1)\n    y_pred_f = y_pred.view(-1)\n    intersection = (y_true_f * y_pred_f).sum()\n    return (2. * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth)","metadata":{"_uuid":"1064f429-a1c3-4d9d-9261-dcc10788a281","_cell_guid":"85ce90e8-b5a9-452f-a1c8-33c6c53a101b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:02.962729Z","iopub.execute_input":"2023-05-12T08:20:02.963113Z","iopub.status.idle":"2023-05-12T08:20:02.969186Z","shell.execute_reply.started":"2023-05-12T08:20:02.963082Z","shell.execute_reply":"2023-05-12T08:20:02.967953Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Loading the Model","metadata":{"_uuid":"1234f690-bc5e-4b78-8ae8-66fe1af77fea","_cell_guid":"09ca65df-2122-4839-83ea-c8809f3880f0","trusted":true}},{"cell_type":"code","source":"# Set up the device for GPU usage\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Define the model\nmodel = SegmentationModel(in_channels).to(device)\n\n# Define the loss function\ncriterion = nn.BCEWithLogitsLoss()\n\n# Define the optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Define the number of epochs\nepochs = 10\n\n# Start the training loop\nfor epoch in range(epochs):\n    print(f'Starting epoch {epoch + 1}/{epochs}')\n    print('-' * 10)\n\n    train_loss = 0.0\n\n    # Set the model to training mode\n    model.train()\n\n    # Iterate over the training data\n    for images, masks in train_dataloader:\n        # Move the images and masks to the GPU\n        images = images.to(device)\n        masks = masks.to(device)\n\n        # Clear the gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(images)\n\n        # Calculate the loss\n        loss = criterion(outputs, masks)\n\n        # Backward pass\n        loss.backward()\n\n        # Update the weights\n        optimizer.step()\n\n        # Update the training loss\n        train_loss += loss.item() * images.size(0)\n\n    # Print the loss for this epoch\n    print(f'Loss: {train_loss / len(train_dataloader):.4f}')\n\nprint('Training complete.')\n\n# Save the trained model\ntorch.save(model.state_dict(), 'segmentation_model.pt')","metadata":{"_uuid":"734100d4-77ee-4b88-a9f5-82af195f9205","_cell_guid":"24d4ef8a-5ebd-4401-9c5f-ec00079a9fd4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:02.972476Z","iopub.execute_input":"2023-05-12T08:20:02.973101Z","iopub.status.idle":"2023-05-12T08:20:32.084425Z","shell.execute_reply.started":"2023-05-12T08:20:02.973060Z","shell.execute_reply":"2023-05-12T08:20:32.082102Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Starting epoch 1/10\n----------\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/surface_volume/42.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/mask/42.tif\nMask shape: (7606, 5249)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/surface_volume/50.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/mask/50.tif\nMask shape: (7606, 5249)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/surface_volume/19.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/mask/19.tif\nMask shape: (8181, 6330)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/surface_volume/54.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/mask/54.tif\nMask shape: (7606, 5249)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/surface_volume/40.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/mask/40.tif\nMask shape: (7606, 5249)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/surface_volume/46.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/mask/46.tif\nMask shape: (8181, 6330)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/surface_volume/14.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/mask/14.tif\nMask shape: (8181, 6330)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/surface_volume/29.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/mask/29.tif\nMask shape: (7606, 5249)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/2/surface_volume/15.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/2/mask/15.tif\nMask shape: (14830, 9506)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/surface_volume/00.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/mask/00.tif\nMask shape: (7606, 5249)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/surface_volume/26.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/mask/26.tif\nMask shape: (8181, 6330)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/2/surface_volume/62.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/2/mask/62.tif\nMask shape: (14830, 9506)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/surface_volume/03.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/mask/03.tif\nMask shape: (7606, 5249)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/2/surface_volume/35.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/2/mask/35.tif\nMask shape: (14830, 9506)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/surface_volume/02.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/mask/02.tif\nMask shape: (8181, 6330)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/surface_volume/07.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/mask/07.tif\nMask shape: (8181, 6330)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Iterate over the training data\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Move the images and masks to the GPU\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m     masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 7606, 5249] at entry 0 and [1, 8181, 6330] at entry 2"],"ename":"RuntimeError","evalue":"stack expects each tensor to be equal size, but got [1, 7606, 5249] at entry 0 and [1, 8181, 6330] at entry 2","output_type":"error"}]},{"cell_type":"markdown","source":"## Prediction","metadata":{"_uuid":"53161053-d1fc-4e13-84f3-0f40c7edd1df","_cell_guid":"d6879bed-df67-47b4-8d79-f5465cbec6f3","trusted":true}},{"cell_type":"code","source":"def predict_image(model, image):\n    \"\"\"Pass an image through the model and return the predicted segmentation.\"\"\"\n    with torch.no_grad():\n        output = model(image)\n        preds = torch.sigmoid(output) > THRESHOLD\n    return preds","metadata":{"_uuid":"96aad7dd-24a5-4642-bb7b-73feac5cbbc0","_cell_guid":"37a76f11-e405-41f1-98c8-344e8a5fa9c4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:32.085237Z","iopub.status.idle":"2023-05-12T08:20:32.085677Z","shell.execute_reply.started":"2023-05-12T08:20:32.085471Z","shell.execute_reply":"2023-05-12T08:20:32.085494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization","metadata":{"_uuid":"013f0893-6290-4472-a5ec-ef333f0b2890","_cell_guid":"cb59ca07-95d9-4a3c-8347-a41b5bdf191c","trusted":true}},{"cell_type":"code","source":"def visualize_prediction(image, prediction):\n    \"\"\"Visualize an image and its predicted segmentation.\"\"\"\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow(image.squeeze(), cmap='gray')\n    plt.title('Original Image')\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow(prediction.squeeze(), cmap='gray')\n    plt.title('Predicted Segmentation')\n    \n    plt.show()","metadata":{"_uuid":"dea86726-86cd-4160-bb0f-581424e52613","_cell_guid":"9d75f0ea-bcc0-4487-b443-a82fe59c3c7c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:32.088184Z","iopub.status.idle":"2023-05-12T08:20:32.088632Z","shell.execute_reply.started":"2023-05-12T08:20:32.088406Z","shell.execute_reply":"2023-05-12T08:20:32.088426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test on a Single Image","metadata":{"_uuid":"37ad89e7-27c0-400f-91da-3fa63f9f9d27","_cell_guid":"0ae98af3-b400-4d91-ac49-69330f5c34dd","trusted":true}},{"cell_type":"code","source":"# Choose a random image from the test dataset\nimage, label, _ = test_dataset[0]\n\n# Make a prediction\nprediction = predict_image(loaded_model, image.unsqueeze(0).to(DEVICE))\n\n# Move the image and prediction to cpu for visualization\nimage = image.cpu()\nprediction = prediction.cpu()\n\n# Visualize the original image and the prediction\nvisualize_prediction(image, prediction)","metadata":{"_uuid":"998dbf77-8aa1-4e3c-baf7-1dfb8d1ce202","_cell_guid":"dd0090c6-9e4f-490e-ba16-6e03629e403a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:32.090157Z","iopub.status.idle":"2023-05-12T08:20:32.090614Z","shell.execute_reply.started":"2023-05-12T08:20:32.090369Z","shell.execute_reply":"2023-05-12T08:20:32.090388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate on Test Set","metadata":{"_uuid":"537cb2f4-afcb-480b-9ffd-3b65c14fb564","_cell_guid":"dc539d8f-9ba7-4e10-950e-ef42eac5a5b9","trusted":true}},{"cell_type":"code","source":"def evaluate_model(model, dataloader):\n    \"\"\"Evaluate the model's performance on a dataloader.\"\"\"\n    model.eval()\n    total = 0\n    correct = 0\n    with torch.no_grad():\n        for images, labels, _ in dataloader:\n            images = images.to(DEVICE)\n            labels = labels.to(DEVICE)\n            \n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return correct / total\n\n# Calculate accuracy on test set\ntest_accuracy = evaluate_model(loaded_model, test_dataloader)\nprint(f'Test accuracy: {test_accuracy}')","metadata":{"_uuid":"068e3a60-51d7-436c-9f1b-feeb02367853","_cell_guid":"f09d1845-07c8-4f88-bc0f-8dbffe5a800f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:32.092024Z","iopub.status.idle":"2023-05-12T08:20:32.092426Z","shell.execute_reply.started":"2023-05-12T08:20:32.092228Z","shell.execute_reply":"2023-05-12T08:20:32.092254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save Predictions for Further Analysis","metadata":{"_uuid":"65b73a91-8d70-4101-8e55-7cdad1266aa7","_cell_guid":"60d28e28-7675-491c-8c59-83f1ce4c28d5","trusted":true}},{"cell_type":"code","source":"def save_predictions(model, dataloader, output_dir):\n    \"\"\"Save the model's predictions on a dataloader to the specified directory.\"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n        \n    model.eval()\n    with torch.no_grad():\n        for i, (images, _, paths) in enumerate(dataloader):\n            images = images.to(DEVICE)\n            outputs = model(images)\n            preds = torch.sigmoid(outputs) > THRESHOLD\n            for j in range(preds.shape[0]):\n                output_path = os.path.join(output_dir, os.path.basename(paths[j]))\n                torchvision.utils.save_image(preds[j], output_path)\n\n# Save predictions on test set\nsave_predictions(loaded_model, test_dataloader, 'predictions/')","metadata":{"_uuid":"3ff8b825-742d-4cd5-83c0-9df84be7cfbe","_cell_guid":"5e08aead-d15b-4c15-a14c-9488ecfdbce5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:32.093830Z","iopub.status.idle":"2023-05-12T08:20:32.094623Z","shell.execute_reply.started":"2023-05-12T08:20:32.094356Z","shell.execute_reply":"2023-05-12T08:20:32.094379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize Predictions","metadata":{"_uuid":"e91a121b-df91-41a4-bfe7-4f58afca037b","_cell_guid":"75a11aa5-04c8-49cc-ab1d-0280b7e87ea7","trusted":true}},{"cell_type":"code","source":"def visualize_predictions(dataset, model, num_samples=5):\n    \"\"\"Visualize model's predictions on a few samples from a dataset.\"\"\"\n    model.eval()\n    samples = random.sample(list(range(len(dataset))), num_samples)\n    with torch.no_grad():\n        for i in samples:\n            image, label, path = dataset[i]\n            image = image.unsqueeze(0).to(DEVICE)\n            output = model(image)\n            pred = torch.sigmoid(output) > THRESHOLD\n            fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n            axs[0].imshow(image[0].cpu().numpy().transpose(1, 2, 0))\n            axs[0].set_title('Input Image')\n            axs[1].imshow(label.cpu().numpy(), cmap='gray')\n            axs[1].set_title('Ground Truth')\n            axs[2].imshow(pred[0].cpu().numpy(), cmap='gray')\n            axs[2].set_title('Predicted Segmentation')\n            for ax in axs:\n                ax.axis('off')\n            plt.show()\n\n# Visualize predictions on test set\nvisualize_predictions(test_dataset, loaded_model)","metadata":{"_uuid":"8b6600b9-718a-449d-b502-8b325307de47","_cell_guid":"9f497621-0a77-4c0f-8764-c36c4305a0d5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:32.096004Z","iopub.status.idle":"2023-05-12T08:20:32.096360Z","shell.execute_reply.started":"2023-05-12T08:20:32.096180Z","shell.execute_reply":"2023-05-12T08:20:32.096197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save Model","metadata":{"_uuid":"3b768d66-f69a-47b2-9fc5-5057da75d687","_cell_guid":"a0c310f4-5982-4cde-a75b-b4d8823045cb","trusted":true}},{"cell_type":"code","source":"torch.save(loaded_model.state_dict(), 'segmentation_model_final.pt')","metadata":{"_uuid":"090c605c-cb00-4026-996d-19d87fdd989e","_cell_guid":"1339579e-45ce-4c79-88d6-8af8861911d5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:32.097410Z","iopub.status.idle":"2023-05-12T08:20:32.097795Z","shell.execute_reply.started":"2023-05-12T08:20:32.097606Z","shell.execute_reply":"2023-05-12T08:20:32.097623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Evaluation Metric","metadata":{"_uuid":"d8c47785-c3cd-409a-89b3-6e790a3b9dbc","_cell_guid":"b0ef0c26-4c33-43c9-942f-55b5745827ea","trusted":true}},{"cell_type":"code","source":"def fbeta_score(y_true, y_pred, beta=0.5):\n    \"\"\"Compute the Fbeta score, a weighted harmonic mean of precision and recall.\"\"\"\n    tp = (y_true * y_pred).sum().to(torch.float32)\n    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n\n    precision = tp / (tp + fp + 1e-12)\n    recall = tp / (tp + fn + 1e-12)\n\n    return (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall + 1e-12)","metadata":{"_uuid":"e3354fa2-a767-41df-b970-48695b27f047","_cell_guid":"81b5e8c1-191b-4687-8473-56debe8269fd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:32.099223Z","iopub.status.idle":"2023-05-12T08:20:32.099659Z","shell.execute_reply.started":"2023-05-12T08:20:32.099429Z","shell.execute_reply":"2023-05-12T08:20:32.099476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate Model","metadata":{"_uuid":"19ce386f-d7bc-4acf-9bda-13390a768864","_cell_guid":"b2929d39-6895-46ec-ba0c-d3bac587ad59","trusted":true}},{"cell_type":"code","source":"def evaluate_model(model, dataset):\n    \"\"\"Evaluate model on a dataset using F0.5 score.\"\"\"\n    model.eval()\n    total_fbeta = 0\n    with torch.no_grad():\n        for image, label, _ in tqdm(dataset):\n            image = image.unsqueeze(0).to(DEVICE)\n            label = label.unsqueeze(0).to(DEVICE)\n            output = model(image)\n            pred = torch.sigmoid(output) > THRESHOLD\n            total_fbeta += fbeta_score(label, pred)\n    return total_fbeta / len(dataset)\n\n# Evaluate model on test set\ntest_fbeta = evaluate_model(loaded_model, test_dataset)\n\n# Print test F0.5 score\nprint(f'Test F0.5 score: {test_fbeta:.4f}')","metadata":{"_uuid":"3023efbf-96a6-4b29-96c1-fe0470eb7b05","_cell_guid":"a351bd89-426f-42c0-a4ca-5d0d72a3cc41","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:32.100852Z","iopub.status.idle":"2023-05-12T08:20:32.101256Z","shell.execute_reply.started":"2023-05-12T08:20:32.101051Z","shell.execute_reply":"2023-05-12T08:20:32.101069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission","metadata":{"_uuid":"177a9689-29e1-4a76-af16-eb606e106fbf","_cell_guid":"e1b90bc9-f0ce-4c8e-b2ac-191d51afe747","trusted":true}},{"cell_type":"code","source":"def rle_encode(img):\n    \"\"\"Perform run-length encoding on binary image.\"\"\"\n    pixels = img.flatten()\n    # We need to add two zeros at the beginning and end to detect runs correctly\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef create_submission(model, dataset, submission_path='submission.csv'):\n    \"\"\"Create submission file for the Kaggle competition.\"\"\"\n    model.eval()\n    with open(submission_path, 'w') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Id', 'Predicted'])\n        with torch.no_grad():\n            for image, _, id_ in tqdm(dataset):\n                image = image.unsqueeze(0).to(DEVICE)\n                output = model(image)\n                pred = torch.sigmoid(output) > THRESHOLD\n                pred_rle = rle_encode(pred.cpu().numpy())\n                writer.writerow([id_, pred_rle])\n\n# Create submission file\ncreate_submission(loaded_model, test_dataset)","metadata":{"_uuid":"38f0b0cf-44ed-4d0f-ba03-d06abd266800","_cell_guid":"054f9ac7-bfe8-413f-bfbc-146d58eb619e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T08:20:32.102424Z","iopub.status.idle":"2023-05-12T08:20:32.102840Z","shell.execute_reply.started":"2023-05-12T08:20:32.102648Z","shell.execute_reply":"2023-05-12T08:20:32.102667Z"},"trusted":true},"execution_count":null,"outputs":[]}]}