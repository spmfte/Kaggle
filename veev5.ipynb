{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries and set paths","metadata":{"_uuid":"243b4407-21b4-4cd5-93ff-86d56a29accb","_cell_guid":"20b17800-ad2b-4d3f-a03c-09b5dc0e2868","trusted":true}},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport numpy as np\nimport cv2\nimport csv\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections.abc import Mapping, Sequence\nfrom six import string_types, integer_types\nfrom torchvision import transforms\nfrom torch import nn\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom tqdm import tqdm\nimport pandas as pd\n\n# Set the path for the data\nDATA_PATH = '/kaggle/input/vesuvius-challenge-ink-detection'\nTRAIN_PATH = os.path.join(DATA_PATH, 'train')\nTEST_PATH = os.path.join(DATA_PATH, 'test')","metadata":{"_uuid":"e9f121a7-5ce9-42c1-9525-cf43b7e8667b","_cell_guid":"067171e0-4599-4144-92ef-89122c8e85b6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T09:15:50.705708Z","iopub.execute_input":"2023-05-12T09:15:50.706110Z","iopub.status.idle":"2023-05-12T09:15:50.860244Z","shell.execute_reply.started":"2023-05-12T09:15:50.706077Z","shell.execute_reply":"2023-05-12T09:15:50.859321Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Constants","metadata":{"_uuid":"c9d40090-a05d-4e10-90f2-b5047d80a20a","_cell_guid":"7fa3c8eb-eea0-4aba-954c-ea5fb6f4ec79","trusted":true}},{"cell_type":"code","source":"PREFIX = '/kaggle/input/vesuvius-challenge-ink-detection/train/1/'\nBUFFER = 30\nZ_START = 27\nZ_DIM = 10\nTRAINING_STEPS = 30000\nNUM_INK_PIXELS = 100\nLEARNING_RATE = 0.001\nBATCH_SIZE = 16\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8ac6a5d3-483b-42c3-911e-81bff0056139","_cell_guid":"8e26ef8e-8fbf-4f9e-94f2-bccc9038c35f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T09:15:50.862942Z","iopub.execute_input":"2023-05-12T09:15:50.863325Z","iopub.status.idle":"2023-05-12T09:15:50.870363Z","shell.execute_reply.started":"2023-05-12T09:15:50.863294Z","shell.execute_reply":"2023-05-12T09:15:50.868913Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Define SubvolumeDataset class","metadata":{"_uuid":"d5b76e83-4252-49d3-b430-1147f6cd5e58","_cell_guid":"d1dc5ffd-bfa6-4990-859a-5330824a7b17","trusted":true}},{"cell_type":"code","source":"import torch\nfrom collections.abc import Mapping, Sequence\nfrom six import string_types\n\ndef default_collate(batch):\n    batch = [item for item in batch if item is not None]  # Filter out None values\n    elem = batch[0]\n    if torch.is_tensor(elem):\n        if len(batch) == 0:\n            raise ValueError(\"batch must contain tensors when it is not empty\")\n        if torch.utils.data.get_worker_info() is not None:\n            return torch.stack(batch, 0, out=torch.zeros(0))\n        else:\n            return torch.stack(batch, 0)\n    elif isinstance(elem, Mapping):\n        return {key: default_collate([d[key] for d in batch]) for key in elem}\n    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n        return type(elem)(*(default_collate(samples) for samples in zip(*batch)))\n    elif isinstance(elem, Sequence) and not isinstance(elem, string_types):\n        transposed = zip(*batch)\n        return [default_collate(samples) for samples in transposed]\n    else:\n        raise TypeError(\"Unsupported batch element type: {}\".format(type(elem)))\n\n\nclass SubvolumeDataset(Dataset):\n    def __init__(self, data_paths, mask_paths, label_paths=None, device='cpu', resize_shape=(256, 256)):\n        self.data_paths = data_paths\n        self.mask_paths = mask_paths\n        self.label_paths = label_paths\n        self.device = device\n        self.resize_shape = resize_shape\n\n    def __len__(self):\n        return len(self.data_paths)\n\n    def __getitem__(self, idx):\n        data_path = self.data_paths[idx]\n        mask_path = self.get_corresponding_mask_path(data_path)\n        print(\"Data path:\", data_path)\n        print(\"Mask path:\", mask_path)\n\n        data = cv2.imread(data_path, cv2.IMREAD_GRAYSCALE)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) if mask_path is not None else None\n\n        if mask is not None:\n            mask_shape = mask.shape\n        else:\n            mask_shape = (256, 256)  # Assign a default shape\n\n        print(\"Mask shape:\", mask_shape)\n\n        # Resize data and mask to the specified shape\n        data = cv2.resize(data, self.resize_shape)\n        mask = cv2.resize(mask, self.resize_shape) if mask is not None else None\n\n        if self.label_paths:\n            label_path = self.get_corresponding_label_path(data_path)\n            label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n        else:\n            label = None\n\n        data_tensor = torch.from_numpy(data).unsqueeze(0).to(self.device)\n        mask_tensor = torch.from_numpy(mask).unsqueeze(0).to(self.device) if mask is not None else None\n        label_tensor = torch.from_numpy(label).unsqueeze(0).to(self.device) if label is not None else None\n\n        resized_image_shape = data_tensor.shape\n        resized_mask_shape = mask_tensor.shape if mask_tensor is not None else mask_shape\n\n        print(\"Resized image shape:\", resized_image_shape)\n        print(\"Resized mask shape:\", resized_mask_shape)\n        print(\"Original image shape:\", data.shape)\n        print(\"Original mask shape:\", mask_shape)\n\n        return data_tensor, mask_tensor, label_tensor\n\n    def get_corresponding_mask_path(self, data_path):\n        # Generate the corresponding mask path\n        mask_path = data_path.replace('surface_volume', 'mask')\n        return mask_path\n\n    def get_corresponding_label_path(self, data_path):\n        # Generate the corresponding label path\n        label_path = data_path.replace('surface_volume', 'inklabels')\n        return label_path","metadata":{"_uuid":"7bd608c1-ae3d-40ec-9299-0b26f668847f","_cell_guid":"74318335-9da0-45bc-b50a-6e8b5629f7fb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T09:15:50.872252Z","iopub.execute_input":"2023-05-12T09:15:50.872740Z","iopub.status.idle":"2023-05-12T09:15:50.899888Z","shell.execute_reply.started":"2023-05-12T09:15:50.872696Z","shell.execute_reply":"2023-05-12T09:15:50.898746Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Initialize Parameters","metadata":{"_uuid":"5e288725-7b60-40f0-8ee5-9b51fd301fca","_cell_guid":"9c3a12cc-3361-4b56-b93a-4bbcddf3fe80","trusted":true}},{"cell_type":"code","source":"# Number of input channels for the model\nin_channels = 1  # Assuming grayscale images, adjust as per your data\n\n# Number of training epochs\nEPOCHS = 10  # Adjust as per your requirement\n\n# Threshold for the RLE encoding\nTHRESHOLD = 0.5  # Adjust based on your model's output","metadata":{"_uuid":"52fc66e6-e153-45c2-9525-b2ec917437b5","_cell_guid":"6b53ed87-8724-464a-ba23-32734cf45870","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T09:15:50.901757Z","iopub.execute_input":"2023-05-12T09:15:50.904771Z","iopub.status.idle":"2023-05-12T09:15:50.917325Z","shell.execute_reply.started":"2023-05-12T09:15:50.904726Z","shell.execute_reply":"2023-05-12T09:15:50.915918Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Define the paths to the training and test data","metadata":{"_uuid":"4b487716-7e80-4d56-952b-25cb71afd439","_cell_guid":"07d556c9-14db-4066-94bf-5c4d0deffef5","trusted":true}},{"cell_type":"code","source":"# For the training data, we have both input (surface volume) and labels (inklabels)\ntrain_data_paths = sorted(glob.glob(os.path.join(TRAIN_PATH, '*/surface_volume/*.tif')))\ntrain_label_paths = sorted(glob.glob(os.path.join(TRAIN_PATH, '*/inklabels.png')))\n\n# For the test data, we only have the input (surface volume)\ntest_data_paths = sorted(glob.glob(os.path.join(TEST_PATH, '*/surface_volume/*.tif')))\n\n# Now, we create the datasets:\ntrain_dataset = SubvolumeDataset(train_data_paths, train_label_paths, device=DEVICE)\ntest_dataset = SubvolumeDataset(test_data_paths, [], device=DEVICE)","metadata":{"_uuid":"c1f97d72-644a-40f1-be08-29c4b26e2a13","_cell_guid":"15db4a65-e51c-4d8b-b126-23993dfe937f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T09:15:50.920508Z","iopub.execute_input":"2023-05-12T09:15:50.920927Z","iopub.status.idle":"2023-05-12T09:15:50.960112Z","shell.execute_reply.started":"2023-05-12T09:15:50.920892Z","shell.execute_reply":"2023-05-12T09:15:50.959110Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Preparation","metadata":{"_uuid":"98a72a2f-dfd0-4536-803b-36cf693667de","_cell_guid":"1c546a5d-cfa8-414d-9354-03bbd8fc4cda","trusted":true}},{"cell_type":"code","source":"vol_dims = (512, 512, 512)  # Replace with the actual dimensions\ntrain_dataset = SubvolumeDataset(train_data_paths, train_label_paths, device=DEVICE)\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\ntest_dataset = SubvolumeDataset(test_data_paths, [], device=DEVICE)","metadata":{"_uuid":"ee42e42a-a5ec-465d-a365-cbc33d730477","_cell_guid":"7e2c5b79-1d3c-4c94-8a3c-d41ac1163d86","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T09:15:50.961593Z","iopub.execute_input":"2023-05-12T09:15:50.962321Z","iopub.status.idle":"2023-05-12T09:15:50.968307Z","shell.execute_reply.started":"2023-05-12T09:15:50.962285Z","shell.execute_reply":"2023-05-12T09:15:50.967023Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Model Def","metadata":{"_uuid":"c66fd147-397c-4f61-b6ea-3f8c027f2457","_cell_guid":"54407f9f-5699-4614-8d2b-b0027b0c21ad","trusted":true}},{"cell_type":"code","source":"class SegmentationModel(nn.Module):\n    def __init__(self, in_channels=1):\n        super(SegmentationModel, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n\n    def forward(self, x):\n        # Assuming x is of shape [batch_size, channels, height, width]\n        x1 = F.relu(self.conv1(x))\n        x2 = F.relu(self.conv2(x1))\n        x3 = F.relu(self.conv3(x2))\n        x4 = F.relu(self.conv4(x3))\n        x5 = F.relu(self.conv5(x4))\n        \n        x6 = F.relu(self.upconv1(x5))\n        x7 = F.relu(self.upconv2(x6))\n        x8 = F.relu(self.upconv3(x7))\n        x9 = F.relu(self.upconv4(x8))\n        \n        output = self.final_conv(x9)\n        \n        return output","metadata":{"_uuid":"7890c1b7-77d1-4e35-9f2a-b52208563486","_cell_guid":"0ca48119-3d29-4154-a214-f119158931c1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T09:15:50.969951Z","iopub.execute_input":"2023-05-12T09:15:50.970557Z","iopub.status.idle":"2023-05-12T09:15:50.988304Z","shell.execute_reply.started":"2023-05-12T09:15:50.970524Z","shell.execute_reply":"2023-05-12T09:15:50.986204Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation Metrics","metadata":{"_uuid":"0ffa259c-6dee-4f47-a85f-0b59750e3e7a","_cell_guid":"04fc24ec-6298-48fe-b6af-b1ca84d9b818","trusted":true}},{"cell_type":"code","source":"def dice_coefficient(y_true, y_pred):\n    smooth = 1e-5\n    y_true_f = y_true.view(-1)\n    y_pred_f = y_pred.view(-1)\n    intersection = (y_true_f * y_pred_f).sum()\n    return (2. * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth)","metadata":{"_uuid":"1064f429-a1c3-4d9d-9261-dcc10788a281","_cell_guid":"85ce90e8-b5a9-452f-a1c8-33c6c53a101b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T09:15:50.990740Z","iopub.execute_input":"2023-05-12T09:15:50.991264Z","iopub.status.idle":"2023-05-12T09:15:51.007964Z","shell.execute_reply.started":"2023-05-12T09:15:50.991221Z","shell.execute_reply":"2023-05-12T09:15:51.006851Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Loading the Model","metadata":{"_uuid":"1234f690-bc5e-4b78-8ae8-66fe1af77fea","_cell_guid":"09ca65df-2122-4839-83ea-c8809f3880f0","trusted":true}},{"cell_type":"code","source":"# Set up the device for GPU usage\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Define the model\nmodel = SegmentationModel(in_channels).to(device)\n\n# Define the loss function\ncriterion = nn.BCEWithLogitsLoss()\n\n# Define the optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Define the number of epochs\nepochs = 10\n\n# Start the training loop\nfor epoch in range(epochs):\n    print(f'Starting epoch {epoch + 1}/{epochs}')\n    print('-' * 10)\n\n    train_loss = 0.0\n\n    # Set the model to training mode\n    model.train()\n\n    # Iterate over the training data\n    for images, masks in train_dataloader:\n        # Move the images and masks to the GPU\n        images = images.to(device)\n        masks = masks.to(device)\n\n        # Clear the gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(images)\n\n        # Calculate the loss\n        loss = criterion(outputs, masks)\n\n        # Backward pass\n        loss.backward()\n\n        # Update the weights\n        optimizer.step()\n\n        # Update the training loss\n        train_loss += loss.item() * images.size(0)\n\n    # Print the loss for this epoch\n    print(f'Loss: {train_loss / len(train_dataloader):.4f}')\n\nprint('Training complete.')\n\n# Save the trained model\ntorch.save(model.state_dict(), 'segmentation_model.pt')","metadata":{"_uuid":"734100d4-77ee-4b88-a9f5-82af195f9205","_cell_guid":"24d4ef8a-5ebd-4401-9c5f-ec00079a9fd4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-05-12T09:15:51.010047Z","iopub.execute_input":"2023-05-12T09:15:51.010920Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Starting epoch 1/10\n----------\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/surface_volume/14.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/mask/14.tif\nMask shape: (256, 256)\nResized image shape: torch.Size([1, 256, 256])\nResized mask shape: (256, 256)\nOriginal image shape: (256, 256)\nOriginal mask shape: (256, 256)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/surface_volume/43.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/mask/43.tif\nMask shape: (256, 256)\nResized image shape: torch.Size([1, 256, 256])\nResized mask shape: (256, 256)\nOriginal image shape: (256, 256)\nOriginal mask shape: (256, 256)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/surface_volume/12.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/mask/12.tif\nMask shape: (256, 256)\nResized image shape: torch.Size([1, 256, 256])\nResized mask shape: (256, 256)\nOriginal image shape: (256, 256)\nOriginal mask shape: (256, 256)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/2/surface_volume/09.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/2/mask/09.tif\nMask shape: (256, 256)\nResized image shape: torch.Size([1, 256, 256])\nResized mask shape: (256, 256)\nOriginal image shape: (256, 256)\nOriginal mask shape: (256, 256)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/surface_volume/57.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/mask/57.tif\nMask shape: (256, 256)\nResized image shape: torch.Size([1, 256, 256])\nResized mask shape: (256, 256)\nOriginal image shape: (256, 256)\nOriginal mask shape: (256, 256)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/surface_volume/39.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/mask/39.tif\nMask shape: (256, 256)\nResized image shape: torch.Size([1, 256, 256])\nResized mask shape: (256, 256)\nOriginal image shape: (256, 256)\nOriginal mask shape: (256, 256)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/surface_volume/42.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/mask/42.tif\nMask shape: (256, 256)\nResized image shape: torch.Size([1, 256, 256])\nResized mask shape: (256, 256)\nOriginal image shape: (256, 256)\nOriginal mask shape: (256, 256)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/surface_volume/41.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/mask/41.tif\nMask shape: (256, 256)\nResized image shape: torch.Size([1, 256, 256])\nResized mask shape: (256, 256)\nOriginal image shape: (256, 256)\nOriginal mask shape: (256, 256)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/surface_volume/25.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/1/mask/25.tif\nMask shape: (256, 256)\nResized image shape: torch.Size([1, 256, 256])\nResized mask shape: (256, 256)\nOriginal image shape: (256, 256)\nOriginal mask shape: (256, 256)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/surface_volume/44.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/3/mask/44.tif\nMask shape: (256, 256)\nResized image shape: torch.Size([1, 256, 256])\nResized mask shape: (256, 256)\nOriginal image shape: (256, 256)\nOriginal mask shape: (256, 256)\nData path: /kaggle/input/vesuvius-challenge-ink-detection/train/2/surface_volume/49.tif\nMask path: /kaggle/input/vesuvius-challenge-ink-detection/train/2/mask/49.tif\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Prediction","metadata":{"_uuid":"53161053-d1fc-4e13-84f3-0f40c7edd1df","_cell_guid":"d6879bed-df67-47b4-8d79-f5465cbec6f3","trusted":true}},{"cell_type":"code","source":"def predict_image(model, image):\n    \"\"\"Pass an image through the model and return the predicted segmentation.\"\"\"\n    with torch.no_grad():\n        output = model(image)\n        preds = torch.sigmoid(output) > THRESHOLD\n    return preds","metadata":{"_uuid":"96aad7dd-24a5-4642-bb7b-73feac5cbbc0","_cell_guid":"37a76f11-e405-41f1-98c8-344e8a5fa9c4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization","metadata":{"_uuid":"013f0893-6290-4472-a5ec-ef333f0b2890","_cell_guid":"cb59ca07-95d9-4a3c-8347-a41b5bdf191c","trusted":true}},{"cell_type":"code","source":"def visualize_prediction(image, prediction):\n    \"\"\"Visualize an image and its predicted segmentation.\"\"\"\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow(image.squeeze(), cmap='gray')\n    plt.title('Original Image')\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow(prediction.squeeze(), cmap='gray')\n    plt.title('Predicted Segmentation')\n    \n    plt.show()","metadata":{"_uuid":"dea86726-86cd-4160-bb0f-581424e52613","_cell_guid":"9d75f0ea-bcc0-4487-b443-a82fe59c3c7c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test on a Single Image","metadata":{"_uuid":"37ad89e7-27c0-400f-91da-3fa63f9f9d27","_cell_guid":"0ae98af3-b400-4d91-ac49-69330f5c34dd","trusted":true}},{"cell_type":"code","source":"# Choose a random image from the test dataset\nimage, label, _ = test_dataset[0]\n\n# Make a prediction\nprediction = predict_image(loaded_model, image.unsqueeze(0).to(DEVICE))\n\n# Move the image and prediction to cpu for visualization\nimage = image.cpu()\nprediction = prediction.cpu()\n\n# Visualize the original image and the prediction\nvisualize_prediction(image, prediction)","metadata":{"_uuid":"998dbf77-8aa1-4e3c-baf7-1dfb8d1ce202","_cell_guid":"dd0090c6-9e4f-490e-ba16-6e03629e403a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate on Test Set","metadata":{"_uuid":"537cb2f4-afcb-480b-9ffd-3b65c14fb564","_cell_guid":"dc539d8f-9ba7-4e10-950e-ef42eac5a5b9","trusted":true}},{"cell_type":"code","source":"def evaluate_model(model, dataloader):\n    \"\"\"Evaluate the model's performance on a dataloader.\"\"\"\n    model.eval()\n    total = 0\n    correct = 0\n    with torch.no_grad():\n        for images, labels, _ in dataloader:\n            images = images.to(DEVICE)\n            labels = labels.to(DEVICE)\n            \n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return correct / total\n\n# Calculate accuracy on test set\ntest_accuracy = evaluate_model(loaded_model, test_dataloader)\nprint(f'Test accuracy: {test_accuracy}')","metadata":{"_uuid":"068e3a60-51d7-436c-9f1b-feeb02367853","_cell_guid":"f09d1845-07c8-4f88-bc0f-8dbffe5a800f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save Predictions for Further Analysis","metadata":{"_uuid":"65b73a91-8d70-4101-8e55-7cdad1266aa7","_cell_guid":"60d28e28-7675-491c-8c59-83f1ce4c28d5","trusted":true}},{"cell_type":"code","source":"def save_predictions(model, dataloader, output_dir):\n    \"\"\"Save the model's predictions on a dataloader to the specified directory.\"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n        \n    model.eval()\n    with torch.no_grad():\n        for i, (images, _, paths) in enumerate(dataloader):\n            images = images.to(DEVICE)\n            outputs = model(images)\n            preds = torch.sigmoid(outputs) > THRESHOLD\n            for j in range(preds.shape[0]):\n                output_path = os.path.join(output_dir, os.path.basename(paths[j]))\n                torchvision.utils.save_image(preds[j], output_path)\n\n# Save predictions on test set\nsave_predictions(loaded_model, test_dataloader, 'predictions/')","metadata":{"_uuid":"3ff8b825-742d-4cd5-83c0-9df84be7cfbe","_cell_guid":"5e08aead-d15b-4c15-a14c-9488ecfdbce5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize Predictions","metadata":{"_uuid":"e91a121b-df91-41a4-bfe7-4f58afca037b","_cell_guid":"75a11aa5-04c8-49cc-ab1d-0280b7e87ea7","trusted":true}},{"cell_type":"code","source":"def visualize_predictions(dataset, model, num_samples=5):\n    \"\"\"Visualize model's predictions on a few samples from a dataset.\"\"\"\n    model.eval()\n    samples = random.sample(list(range(len(dataset))), num_samples)\n    with torch.no_grad():\n        for i in samples:\n            image, label, path = dataset[i]\n            image = image.unsqueeze(0).to(DEVICE)\n            output = model(image)\n            pred = torch.sigmoid(output) > THRESHOLD\n            fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n            axs[0].imshow(image[0].cpu().numpy().transpose(1, 2, 0))\n            axs[0].set_title('Input Image')\n            axs[1].imshow(label.cpu().numpy(), cmap='gray')\n            axs[1].set_title('Ground Truth')\n            axs[2].imshow(pred[0].cpu().numpy(), cmap='gray')\n            axs[2].set_title('Predicted Segmentation')\n            for ax in axs:\n                ax.axis('off')\n            plt.show()\n\n# Visualize predictions on test set\nvisualize_predictions(test_dataset, loaded_model)","metadata":{"_uuid":"8b6600b9-718a-449d-b502-8b325307de47","_cell_guid":"9f497621-0a77-4c0f-8764-c36c4305a0d5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save Model","metadata":{"_uuid":"3b768d66-f69a-47b2-9fc5-5057da75d687","_cell_guid":"a0c310f4-5982-4cde-a75b-b4d8823045cb","trusted":true}},{"cell_type":"code","source":"torch.save(loaded_model.state_dict(), 'segmentation_model_final.pt')","metadata":{"_uuid":"090c605c-cb00-4026-996d-19d87fdd989e","_cell_guid":"1339579e-45ce-4c79-88d6-8af8861911d5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Evaluation Metric","metadata":{"_uuid":"d8c47785-c3cd-409a-89b3-6e790a3b9dbc","_cell_guid":"b0ef0c26-4c33-43c9-942f-55b5745827ea","trusted":true}},{"cell_type":"code","source":"def fbeta_score(y_true, y_pred, beta=0.5):\n    \"\"\"Compute the Fbeta score, a weighted harmonic mean of precision and recall.\"\"\"\n    tp = (y_true * y_pred).sum().to(torch.float32)\n    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n\n    precision = tp / (tp + fp + 1e-12)\n    recall = tp / (tp + fn + 1e-12)\n\n    return (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall + 1e-12)","metadata":{"_uuid":"e3354fa2-a767-41df-b970-48695b27f047","_cell_guid":"81b5e8c1-191b-4687-8473-56debe8269fd","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate Model","metadata":{"_uuid":"19ce386f-d7bc-4acf-9bda-13390a768864","_cell_guid":"b2929d39-6895-46ec-ba0c-d3bac587ad59","trusted":true}},{"cell_type":"code","source":"def evaluate_model(model, dataset):\n    \"\"\"Evaluate model on a dataset using F0.5 score.\"\"\"\n    model.eval()\n    total_fbeta = 0\n    with torch.no_grad():\n        for image, label, _ in tqdm(dataset):\n            image = image.unsqueeze(0).to(DEVICE)\n            label = label.unsqueeze(0).to(DEVICE)\n            output = model(image)\n            pred = torch.sigmoid(output) > THRESHOLD\n            total_fbeta += fbeta_score(label, pred)\n    return total_fbeta / len(dataset)\n\n# Evaluate model on test set\ntest_fbeta = evaluate_model(loaded_model, test_dataset)\n\n# Print test F0.5 score\nprint(f'Test F0.5 score: {test_fbeta:.4f}')","metadata":{"_uuid":"3023efbf-96a6-4b29-96c1-fe0470eb7b05","_cell_guid":"a351bd89-426f-42c0-a4ca-5d0d72a3cc41","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission","metadata":{"_uuid":"177a9689-29e1-4a76-af16-eb606e106fbf","_cell_guid":"e1b90bc9-f0ce-4c8e-b2ac-191d51afe747","trusted":true}},{"cell_type":"code","source":"def rle_encode(img):\n    \"\"\"Perform run-length encoding on binary image.\"\"\"\n    pixels = img.flatten()\n    # We need to add two zeros at the beginning and end to detect runs correctly\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef create_submission(model, dataset, submission_path='submission.csv'):\n    \"\"\"Create submission file for the Kaggle competition.\"\"\"\n    model.eval()\n    with open(submission_path, 'w') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Id', 'Predicted'])\n        with torch.no_grad():\n            for image, _, id_ in tqdm(dataset):\n                image = image.unsqueeze(0).to(DEVICE)\n                output = model(image)\n                pred = torch.sigmoid(output) > THRESHOLD\n                pred_rle = rle_encode(pred.cpu().numpy())\n                writer.writerow([id_, pred_rle])\n\n# Create submission file\ncreate_submission(loaded_model, test_dataset)","metadata":{"_uuid":"38f0b0cf-44ed-4d0f-ba03-d06abd266800","_cell_guid":"054f9ac7-bfe8-413f-bfbc-146d58eb619e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}