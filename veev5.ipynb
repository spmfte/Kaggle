{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries and set paths","metadata":{"_uuid":"fefd08e3-72f9-47e4-96c5-329bca0cbcdf","_cell_guid":"99ff5808-186f-4d90-8bd5-560778d15037","trusted":true}},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch import nn\nimport torch.nn.functional as F\nfrom PIL import Image\nfrom tqdm import tqdm\nimport pandas as pd\n\n# Set the path for the data\nDATA_PATH = '/kaggle/input/vesuvius-challenge-ink-detection'\nTRAIN_PATH = os.path.join(DATA_PATH, 'train')\nTEST_PATH = os.path.join(DATA_PATH, 'test')","metadata":{"_uuid":"e57fc414-2e4d-48da-b8a4-a45ca12cff9d","_cell_guid":"151df5cb-165b-49cb-b874-21ad64feb400","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Constants","metadata":{"_uuid":"6bdedc1b-fdf5-4f66-909c-3251a690250b","_cell_guid":"11ab1322-d5de-4fc1-94fd-97bd2672a32a","trusted":true}},{"cell_type":"code","source":"PREFIX = '/kaggle/input/vesuvius-challenge-ink-detection/train/1/'\nBUFFER = 30\nZ_START = 27\nZ_DIM = 10\nTRAINING_STEPS = 30000\nNUM_INK_PIXELS = 100\nLEARNING_RATE = 0.001\nBATCH_SIZE = 16\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"bacb64bf-f967-406d-8bd6-cf0c55d04f2a","_cell_guid":"a24430ec-354d-47de-ba50-a742661cae2d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define SubvolumeDataset class","metadata":{"_uuid":"6ec6e60d-7777-45d6-944b-1e53cbd6f464","_cell_guid":"446904c4-8eca-4c9c-8bd0-67b4ce503967","trusted":true}},{"cell_type":"code","source":"class SubvolumeDataset(Dataset):\n    def __init__(self, data_paths, mask_paths, label_paths=None, device='cpu'):\n        self.data_paths = data_paths\n        self.mask_paths = mask_paths\n        self.label_paths = label_paths\n        self.device = device\n\n    def __len__(self):\n        return len(self.data_paths)\n\n    def __getitem__(self, idx):\n        data_path = self.data_paths[idx]\n        mask_path = self.get_corresponding_mask_path(data_path)\n\n        data = cv2.imread(data_path, cv2.IMREAD_GRAYSCALE)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n\n        if self.label_paths:\n            label_path = self.get_corresponding_label_path(data_path)\n            label = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n        else:\n            label = None\n\n        return (torch.from_numpy(data).to(self.device), \n                torch.from_numpy(mask).to(self.device), \n                torch.from_numpy(label).to(self.device) if label is not None else None)\n\n    def get_corresponding_mask_path(self, data_path):\n        # Generate the corresponding mask path\n        mask_path = data_path.replace('surface_volume', 'mask')\n        return mask_path\n\n    def get_corresponding_label_path(self, data_path):\n        # Generate the corresponding label path\n        label_path = data_path.replace('surface_volume', 'inklabels')\n        return label_path","metadata":{"_uuid":"60757fc3-b679-4ee1-ba0d-7e4ae6ad5f72","_cell_guid":"cf631fbb-29ea-4f39-b5d7-881532df998a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initialize Parameters","metadata":{"_uuid":"5fcdd5ea-cf58-458a-94bc-7ee8f2c4eaa0","_cell_guid":"e5de8f4c-5481-4d0c-b532-83fededc257e","trusted":true}},{"cell_type":"code","source":"# Number of input channels for the model\nin_channels = 1  # Assuming grayscale images, adjust as per your data\n\n# Number of training epochs\nEPOCHS = 10  # Adjust as per your requirement\n\n# Threshold for the RLE encoding\nTHRESHOLD = 0.5  # Adjust based on your model's output","metadata":{"_uuid":"dfb35947-f71a-48cf-8121-9424837c58bc","_cell_guid":"df76a1f6-07d6-4131-9ba2-5e1c731fffca","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the paths to the training and test data","metadata":{"_uuid":"e3c05e21-b4b0-4399-9b3a-f1a91bb064a6","_cell_guid":"d13378a0-92f7-4f22-a73a-042403638582","trusted":true}},{"cell_type":"code","source":"train_data_paths = sorted(glob.glob(os.path.join(TRAIN_PATH, '*/surface_volume/*.tif')))\ntrain_label_paths = sorted(glob.glob(os.path.join(TRAIN_PATH, '*/label_volume/*.tif')))\ntrain_ink_pixels_paths = sorted(glob.glob(os.path.join(TRAIN_PATH, '*/ink_pixels/*.csv')))\n\ntest_data_paths = sorted(glob.glob(os.path.join(TEST_PATH, '*/surface_volume/*.tif')))\ntest_label_paths = sorted(glob.glob(os.path.join(TEST_PATH, '*/label_volume/*.tif')))\ntest_ink_pixels_paths = sorted(glob.glob(os.path.join(TEST_PATH, '*/ink_pixels/*.csv')))","metadata":{"_uuid":"d713924a-54f8-43bb-acbf-6809e0368180","_cell_guid":"a31820c6-4554-4f14-8d6e-7c847e1a73a9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Preparation","metadata":{"_uuid":"7ae4cabd-6b07-4b19-8b42-d209f895959d","_cell_guid":"f8a47057-7e67-471b-800c-f3ec1bcd567c","trusted":true}},{"cell_type":"code","source":"vol_dims = (512, 512, 512)  # Replace with the actual dimensions\ntrain_dataset = SubvolumeDataset(train_data_paths, train_label_paths, train_ink_pixels_paths, vol_dims, DEVICE)\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\ntest_dataset = SubvolumeDataset(test_data_paths, test_label_paths, test_ink_pixels_paths, vol_dims, DEVICE)","metadata":{"_uuid":"3bd7d076-c776-41e4-b7a9-ac79e5368637","_cell_guid":"49e63bb6-ee57-4505-b4e6-5357440b7db0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Def","metadata":{"_uuid":"e8eeae90-479c-4352-bdba-6377d05eeac1","_cell_guid":"e3c27361-bc1b-4fb4-9fd6-cefc3097c061","trusted":true}},{"cell_type":"code","source":"class SegmentationModel(nn.Module):\n    def __init__(self, in_channels=1):\n        super(SegmentationModel, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.conv5 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.final_conv = nn.Conv2d(64, 1, kernel_size=1)\n\n    def forward(self, x):\n        # Assuming x is of shape [batch_size, channels, height, width]\n        x1 = F.relu(self.conv1(x))\n        x2 = F.relu(self.conv2(x1))\n        x3 = F.relu(self.conv3(x2))\n        x4 = F.relu(self.conv4(x3))\n        x5 = F.relu(self.conv5(x4))\n        \n        x6 = F.relu(self.upconv1(x5))\n        x7 = F.relu(self.upconv2(x6))\n        x8 = F.relu(self.upconv3(x7))\n        x9 = F.relu(self.upconv4(x8))\n        \n        output = self.final_conv(x9)\n        \n        return output","metadata":{"_uuid":"822b60b7-12d1-4bdd-9121-f9aa30ab19ae","_cell_guid":"00d358e6-588f-4b81-a8a2-aa44d195676a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation Metrics","metadata":{"_uuid":"d52d51d1-079f-4a12-ab6b-48809123f1a1","_cell_guid":"066bbb77-f035-4a33-bdf7-80a11bd148a3","trusted":true}},{"cell_type":"code","source":"def dice_coefficient(y_true, y_pred):\n    smooth = 1e-5\n    y_true_f = y_true.view(-1)\n    y_pred_f = y_pred.view(-1)\n    intersection = (y_true_f * y_pred_f).sum()\n    return (2. * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth)","metadata":{"_uuid":"2ee4a9b0-7659-4e5b-89ff-516c472f10d6","_cell_guid":"bed7b4c8-88f6-4a0a-8a11-f4fbdee179b7","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving the Model","metadata":{"_uuid":"3ef7c916-9172-4e89-a29e-08e5218756ac","_cell_guid":"8a2d12c8-af41-44a7-8647-a3d92ff8373d","trusted":true}},{"cell_type":"markdown","source":"## Loading the Model","metadata":{"_uuid":"3159f077-89b7-48a6-8845-200a26865662","_cell_guid":"c4ee610b-0aa4-4933-a495-4e6eb4364133","trusted":true}},{"cell_type":"code","source":"# Set up the device for GPU usage\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Define the model\nmodel = SegmentationModel(in_channels).to(device)\n\n# Define the loss function\ncriterion = nn.BCEWithLogitsLoss()\n\n# Define the optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Define the number of epochs\nepochs = 10\n\n# Start the training loop\nfor epoch in range(epochs):\n    print(f'Starting epoch {epoch + 1}/{epochs}')\n    print('-' * 10)\n\n    train_loss = 0.0\n\n    # Set the model to training mode\n    model.train()\n\n    # Iterate over the training data\n    for images, masks in train_dataloader:\n        # Move the images and masks to the GPU\n        images = images.to(device)\n        masks = masks.to(device)\n\n        # Clear the gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(images)\n\n        # Calculate the loss\n        loss = criterion(outputs, masks)\n\n        # Backward pass\n        loss.backward()\n\n        # Update the weights\n        optimizer.step()\n\n        # Update the training loss\n        train_loss += loss.item() * images.size(0)\n\n    # Print the loss for this epoch\n    print(f'Loss: {train_loss / len(train_dataloader):.4f}')\n\nprint('Training complete.')\n\n# Save the trained model\ntorch.save(model.state_dict(), 'segmentation_model.pt')","metadata":{"_uuid":"857c2beb-351b-4b9f-974f-88bb99f40059","_cell_guid":"59cfe42c-b352-40d4-a3bc-70f57bc4c4cc","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction","metadata":{"_uuid":"473ccac0-26a7-4bea-a4f4-bd6838f09a92","_cell_guid":"341d6f19-be11-4670-93a7-bafa0737c6e7","trusted":true}},{"cell_type":"code","source":"def predict_image(model, image):\n    \"\"\"Pass an image through the model and return the predicted segmentation.\"\"\"\n    with torch.no_grad():\n        output = model(image)\n        preds = torch.sigmoid(output) > THRESHOLD\n    return preds","metadata":{"_uuid":"faff2907-5dc1-430f-b49d-2d5906ba6599","_cell_guid":"d1f4f698-1a4d-4cbc-910e-35459824fc2c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization","metadata":{"_uuid":"d9c4a071-2e77-4004-bd1b-e19e8188bbd6","_cell_guid":"c742a22d-4995-4427-813e-f6a4c37b601d","trusted":true}},{"cell_type":"code","source":"def visualize_prediction(image, prediction):\n    \"\"\"Visualize an image and its predicted segmentation.\"\"\"\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.imshow(image.squeeze(), cmap='gray')\n    plt.title('Original Image')\n    \n    plt.subplot(1, 2, 2)\n    plt.imshow(prediction.squeeze(), cmap='gray')\n    plt.title('Predicted Segmentation')\n    \n    plt.show()","metadata":{"_uuid":"704376c2-759f-464b-941c-c5147b6c786b","_cell_guid":"752d649b-cfb8-49c5-9fa6-1884e1d70452","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test on a Single Image","metadata":{"_uuid":"64f49784-0fc1-42af-8bb6-44cbedac85ac","_cell_guid":"1c0066eb-de34-40ff-abe5-61fe6e27c0a5","trusted":true}},{"cell_type":"code","source":"# Choose a random image from the test dataset\nimage, label, _ = test_dataset[0]\n\n# Make a prediction\nprediction = predict_image(loaded_model, image.unsqueeze(0).to(DEVICE))\n\n# Move the image and prediction to cpu for visualization\nimage = image.cpu()\nprediction = prediction.cpu()\n\n# Visualize the original image and the prediction\nvisualize_prediction(image, prediction)","metadata":{"_uuid":"ac541efa-960d-46f5-9b85-ba80b8e42ef9","_cell_guid":"8799f61f-77b1-4363-a04e-22e4b4c71e9b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate on Test Set","metadata":{"_uuid":"112e2f5a-888c-4d7e-9855-54090097bbeb","_cell_guid":"5419857b-6639-4c54-8650-128c1806124f","trusted":true}},{"cell_type":"code","source":"def evaluate_model(model, dataloader):\n    \"\"\"Evaluate the model's performance on a dataloader.\"\"\"\n    model.eval()\n    total = 0\n    correct = 0\n    with torch.no_grad():\n        for images, labels, _ in dataloader:\n            images = images.to(DEVICE)\n            labels = labels.to(DEVICE)\n            \n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return correct / total\n\n# Calculate accuracy on test set\ntest_accuracy = evaluate_model(loaded_model, test_dataloader)\nprint(f'Test accuracy: {test_accuracy}')","metadata":{"_uuid":"b0015715-0a55-4dd9-bd94-2584fb78b704","_cell_guid":"e66f88cd-d58d-4b60-be85-74859e89495f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save Predictions for Further Analysis","metadata":{"_uuid":"14feadef-5e85-4c48-ace7-16437fca6c7f","_cell_guid":"4a6dfca9-0df1-42d2-aa98-9e674fc197a7","trusted":true}},{"cell_type":"code","source":"def save_predictions(model, dataloader, output_dir):\n    \"\"\"Save the model's predictions on a dataloader to the specified directory.\"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n        \n    model.eval()\n    with torch.no_grad():\n        for i, (images, _, paths) in enumerate(dataloader):\n            images = images.to(DEVICE)\n            outputs = model(images)\n            preds = torch.sigmoid(outputs) > THRESHOLD\n            for j in range(preds.shape[0]):\n                output_path = os.path.join(output_dir, os.path.basename(paths[j]))\n                torchvision.utils.save_image(preds[j], output_path)\n\n# Save predictions on test set\nsave_predictions(loaded_model, test_dataloader, 'predictions/')","metadata":{"_uuid":"bca6f9c0-d02d-45d8-8f28-cf49186100ee","_cell_guid":"8af2c30a-934f-43b4-8d84-e149d581a039","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize Predictions","metadata":{"_uuid":"71e2bbe1-012b-45b4-889d-ea79e37dc93a","_cell_guid":"7ffa893a-2506-4a8d-9dd4-85cc9ea59813","trusted":true}},{"cell_type":"code","source":"def visualize_predictions(dataset, model, num_samples=5):\n    \"\"\"Visualize model's predictions on a few samples from a dataset.\"\"\"\n    model.eval()\n    samples = random.sample(list(range(len(dataset))), num_samples)\n    with torch.no_grad():\n        for i in samples:\n            image, label, path = dataset[i]\n            image = image.unsqueeze(0).to(DEVICE)\n            output = model(image)\n            pred = torch.sigmoid(output) > THRESHOLD\n            fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n            axs[0].imshow(image[0].cpu().numpy().transpose(1, 2, 0))\n            axs[0].set_title('Input Image')\n            axs[1].imshow(label.cpu().numpy(), cmap='gray')\n            axs[1].set_title('Ground Truth')\n            axs[2].imshow(pred[0].cpu().numpy(), cmap='gray')\n            axs[2].set_title('Predicted Segmentation')\n            for ax in axs:\n                ax.axis('off')\n            plt.show()\n\n# Visualize predictions on test set\nvisualize_predictions(test_dataset, loaded_model)","metadata":{"_uuid":"35fbfd1a-7c69-42b3-8843-8417ea379b8a","_cell_guid":"33ea0aea-24bb-4753-a165-4c28d947d60c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save Model","metadata":{"_uuid":"c50290aa-f526-43b6-9303-776fa4ff422e","_cell_guid":"f126038f-1b42-41b5-a873-589b6ab5109c","trusted":true}},{"cell_type":"code","source":"torch.save(loaded_model.state_dict(), 'segmentation_model_final.pt')","metadata":{"_uuid":"c63b154e-d4d6-4a74-aa0b-d0e77704de0b","_cell_guid":"a123e90c-56e9-4507-81e2-ebcb7d68d9f4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Evaluation Metric","metadata":{"_uuid":"104ea58f-61b1-4399-9506-b50890f3c1ba","_cell_guid":"4d99cd42-c1d0-4f2b-b325-427a6f1af0a3","trusted":true}},{"cell_type":"code","source":"def fbeta_score(y_true, y_pred, beta=0.5):\n    \"\"\"Compute the Fbeta score, a weighted harmonic mean of precision and recall.\"\"\"\n    tp = (y_true * y_pred).sum().to(torch.float32)\n    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n\n    precision = tp / (tp + fp + 1e-12)\n    recall = tp / (tp + fn + 1e-12)\n\n    return (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall + 1e-12)","metadata":{"_uuid":"96b36a9d-06ae-4ec0-a914-677ff91556d9","_cell_guid":"17a02c7c-ef4e-4add-a7fd-2f781648fcf0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate Model","metadata":{"_uuid":"44a440db-d5e3-4da2-9ef2-33c92ac40954","_cell_guid":"de4f1c2c-6f3c-40a4-86a8-929754e1bce1","trusted":true}},{"cell_type":"code","source":"def evaluate_model(model, dataset):\n    \"\"\"Evaluate model on a dataset using F0.5 score.\"\"\"\n    model.eval()\n    total_fbeta = 0\n    with torch.no_grad():\n        for image, label, _ in tqdm(dataset):\n            image = image.unsqueeze(0).to(DEVICE)\n            label = label.unsqueeze(0).to(DEVICE)\n            output = model(image)\n            pred = torch.sigmoid(output) > THRESHOLD\n            total_fbeta += fbeta_score(label, pred)\n    return total_fbeta / len(dataset)\n\n# Evaluate model on test set\ntest_fbeta = evaluate_model(loaded_model, test_dataset)\n\n# Print test F0.5 score\nprint(f'Test F0.5 score: {test_fbeta:.4f}')","metadata":{"_uuid":"65839ecd-0a7c-4161-ac9f-9b362b736f62","_cell_guid":"6a2244d5-bab9-491d-b791-8ac2eec24e7d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission","metadata":{"_uuid":"39332e78-8890-44bb-9d24-524cbaa0fe0b","_cell_guid":"05099537-3803-430b-946e-4d37ecd2a5fc","trusted":true}},{"cell_type":"code","source":"def rle_encode(img):\n    \"\"\"Perform run-length encoding on binary image.\"\"\"\n    pixels = img.flatten()\n    # We need to add two zeros at the beginning and end to detect runs correctly\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef create_submission(model, dataset, submission_path='submission.csv'):\n    \"\"\"Create submission file for the Kaggle competition.\"\"\"\n    model.eval()\n    with open(submission_path, 'w') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Id', 'Predicted'])\n        with torch.no_grad():\n            for image, _, id_ in tqdm(dataset):\n                image = image.unsqueeze(0).to(DEVICE)\n                output = model(image)\n                pred = torch.sigmoid(output) > THRESHOLD\n                pred_rle = rle_encode(pred.cpu().numpy())\n                writer.writerow([id_, pred_rle])\n\n# Create submission file\ncreate_submission(loaded_model, test_dataset)","metadata":{"_uuid":"c152d52c-cae5-4518-80e5-ee3213fa31e9","_cell_guid":"0d0e73d1-3937-4ccf-a745-13d0fb52c05b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}