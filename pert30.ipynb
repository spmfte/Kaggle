{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pyarrow.parquet as pq\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\n\n# Define the path to the dataset directory\nDATA_DIR = \"/kaggle/input/open-problems-single-cell-perturbations\"\n\n# Function to read the dataset in chunks\ndef read_in_chunks(file_path, chunk_size=10000):\n    if file_path.endswith('.parquet'):\n        parquet_file = pq.ParquetFile(file_path)\n        num_rows = parquet_file.metadata.num_rows\n        num_chunks = num_rows // chunk_size + 1\n\n        for i in range(num_chunks):\n            table = parquet_file.read_row_group(i, columns=None)\n            yield table.to_pandas()\n    elif file_path.endswith('.csv'):\n        for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n            yield chunk","metadata":{"_uuid":"7b575297-32e1-4d21-ab73-817900926e53","_cell_guid":"a3899b3e-7985-483d-b155-33283f5b5084","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-15T05:41:39.069546Z","iopub.execute_input":"2023-09-15T05:41:39.070039Z","iopub.status.idle":"2023-09-15T05:41:43.071709Z","shell.execute_reply.started":"2023-09-15T05:41:39.069992Z","shell.execute_reply":"2023-09-15T05:41:43.070749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Exploration","metadata":{"_uuid":"e52c9df6-1fde-4244-89ea-afdaa1b2b1ea","_cell_guid":"12ddc7ed-8b16-4233-9e0d-b57915ed0eda","trusted":true}},{"cell_type":"code","source":"datasets = {\n    \"adata_obs_meta\": f\"{DATA_DIR}/adata_obs_meta.csv\",\n    \"adata_train\": f\"{DATA_DIR}/adata_train.parquet\",\n    \"de_train\": f\"{DATA_DIR}/de_train.parquet\",\n    \"id_map\": f\"{DATA_DIR}/id_map.csv\",\n    \"multiome_obs_meta\": f\"{DATA_DIR}/multiome_obs_meta.csv\",\n    \"multiome_train\": f\"{DATA_DIR}/multiome_train.parquet\",\n    \"multiome_var_meta\": f\"{DATA_DIR}/multiome_var_meta.csv\"\n}\n\nfor name, path in datasets.items():\n    data_chunk_gen = read_in_chunks(path)\n    data_first_chunk = next(data_chunk_gen)\n    print(f\"--- {name.upper()} ---\")\n    print(data_first_chunk.head())\n\n# Visualization and summary stats\nsample_data = next(read_in_chunks(datasets[\"de_train\"], chunk_size=1000))\nsample_data.hist(bins=50, figsize=(20, 15))\nplt.show()","metadata":{"_uuid":"1b330c92-34d5-4113-8aef-8508db1bf3d7","_cell_guid":"c5a1a14c-95e7-4dd5-a336-ac20f3e3dd37","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-09-15T05:41:43.073659Z","iopub.execute_input":"2023-09-15T05:41:43.074336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{"_uuid":"193833b4-be92-4706-9439-fadbf0a3d09a","_cell_guid":"4c514f5f-c5e5-4b00-b356-fe28ac1b220c","trusted":true}},{"cell_type":"code","source":"scaler = MinMaxScaler()\n\ndef preprocess_data(chunk):\n    chunk = handle_missing_data(chunk)\n    scaled_data = scaler.fit_transform(chunk.iloc[:, :-1])  # Assuming the last column is the target\n    return pd.DataFrame(scaled_data, columns=chunk.columns[:-1])\n\ndef handle_missing_data(chunk):\n    for column in chunk.columns:\n        median_val = chunk[column].median()\n        chunk[column].fillna(median_val, inplace=True)\n    return chunk","metadata":{"_uuid":"093c2ed9-c5d9-4781-a473-dacd7b5e9ba8","_cell_guid":"989ebc30-45c7-4836-ae01-2a00ef846913","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Development","metadata":{"_uuid":"ea2f3ad4-1b66-4d7e-ad2d-63e3005b7f7d","_cell_guid":"2d034f02-7e9b-4113-aacb-7f5f27c2c1a0","trusted":true}},{"cell_type":"code","source":"params = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'boosting_type': 'gbdt'\n}\n\nde_train_gen = read_in_chunks(datasets[\"de_train\"])\nfirst_chunk = next(de_train_gen)\ntrain_data = preprocess_data(first_chunk)\nX = train_data.drop('target_column', axis=1)  # replace 'target_column' with the actual column name\ny = first_chunk['target_column']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n\ntrain_dataset = lgb.Dataset(X_train, label=y_train)\nval_dataset = lgb.Dataset(X_val, label=y_val, reference=train_dataset)\n\nmodel = lgb.train(params, train_dataset, valid_sets=val_dataset, num_boost_round=500, early_stopping_rounds=50)\nmodel.save_model('model.txt')","metadata":{"_uuid":"1b778375-9872-4f05-a5a2-e2bd3dcf46b2","_cell_guid":"8691ffdb-b3c1-4499-8e24-f48547ec1193","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{"_uuid":"d6d8a5ca-05b2-4110-bf54-46e192586722","_cell_guid":"1033bb1e-448a-40ac-923d-93e55e2cd91f","trusted":true}},{"cell_type":"code","source":"# Assuming de_train dataset will also be split for evaluation\nX_test = preprocess_data(first_chunk).drop('target_column', axis=1)  # replace 'target_column' with the actual column name\ny_test = first_chunk['target_column']\n\npredictions = model.predict(X_test)\nmse = ((predictions - y_test) ** 2).mean()\nrmse = np.sqrt(mse)\nprint(f\"RMSE: {rmse}\")","metadata":{"_uuid":"43b1db83-0621-4ecf-9cf5-1521e3c6ce04","_cell_guid":"8fff4e2f-5bc7-4d95-96e5-22f3d4b35d10","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}